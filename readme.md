# Syst√®me de Pr√©diction de Dur√©e de Trajets Urbains

Un pipeline complet de Machine Learning de bout en bout pour pr√©dire la dur√©e des trajets de taxi urbains, int√©grant le traitement distribu√© de donn√©es, l'entra√Ænement de mod√®les ML et le d√©ploiement d'API s√©curis√©e.


## 1. Aper√ßu du Projet

Ce projet construit un syst√®me complet de traitement de donn√©es et de machine learning pour une start-up de logistique urbaine afin d'am√©liorer les pr√©dictions de Temps d'Arriv√©e Estim√© (ETA). Le syst√®me g√®re l'ingestion de donn√©es, le nettoyage, l'ing√©nierie de features, l'entra√Ænement de mod√®les et fournit des pr√©dictions via une API s√©curis√©e.

### Contexte M√©tier

Une start-up de logistique urbaine souhaite am√©liorer la visibilit√© sur les temps d'arriv√©e. Ce syst√®me fournit :

- Pipeline automatis√© d'ingestion et de traitement des donn√©es
- Pr√©dictions ETA en temps r√©el
- Analyses avanc√©es des patterns de trajets
- Acc√®s API s√©curis√© avec authentification JWT
- Monitoring et logging complets

## 2. Architecture

Le projet suit une architecture m√©daillon avec trois zones de donn√©es :

1. **Couche Bronze** : Donn√©es brutes, non trait√©es
2. **Couche Silver** : Donn√©es nettoy√©es, normaliser et enrichies stock√©es dans PostgreSQL
3. **Couche ML** : Mod√®les entra√Æn√©s et pr√©dictions

### Flux de Donn√©es

```
Donn√©es Brutes ‚Üí DAG Airflow ‚Üí Traitement PySpark ‚Üí PostgreSQL ‚Üí FastAPI ‚Üí Client
```

## Dataset

**Source** : Enregistrements des trajets de taxis NYC

### Caract√©ristiques Principales

| Colonne | Description |
|---------|-------------|
| `VendorID` | Identifiant du fournisseur de taxi (1 ou 2) |
| `tpep_pickup_datetime` | Date et heure de d√©but du trajet |
| `tpep_dropoff_datetime` | Date et heure de fin du trajet |
| `passenger_count` | Nombre de passagers |
| `trip_distance` | Distance du trajet en miles |
| `RatecodeID` | Code du type de tarif (1=Standard, 2=Sp√©cial, 3=Newark, etc.) |
| `PULocationID` | Identifiant de la zone de d√©part (pickup) |
| `DOLocationID` | Identifiant de la zone d'arriv√©e (dropoff) |
| `payment_type` | Type de paiement (0=Esp√®ces, 1=Carte, etc.) |
| `fare_amount` | Tarif de base |
| `total_amount` | Montant total pay√© |

### Variable Cible

**`duration_minutes`** : Calcul√©e comme `tpep_dropoff_datetime - tpep_pickup_datetime`

## ‚ú® Fonctionnalit√©s

### Traitement des Donn√©es

- **D√©tection des Valeurs Aberrantes** : Filtrage bas√© sur IQR pour les trajets aberrants
- **Validation des Donn√©es** : 
  - Distance : 0 < distance ‚â§ 200 miles
  - Dur√©e : > 0 minutes
  - Passagers : > 0
- **Ing√©nierie de Features** :
  - `pickup_hour` : Heure de la journ√©e (0-23)
  - `day_of_week` : Jour de la semaine (0-6)
  - `month` : Mois de l'ann√©e (1-12)

### Machine Learning

- **Algorithme** : Random Forest Regressor
- **Cible** : Dur√©e du trajet en minutes
- **M√©triques d'√âvaluation** : RMSE, R¬≤

### API & Analytics

- **Endpoint de Pr√©diction** : Pr√©dictions ETA en temps r√©el
- **Analytics Avanc√©es** : Analyses bas√©es sur SQL utilisant des CTEs
- **S√©curit√©** : Authentification JWT
- **Monitoring** : Logging des pr√©dictions pour suivi du mod√®le

## Stack Technique

- **Orchestration** : Apache Airflow
- **Traitement de Donn√©es** : PySpark
- **Base de Donn√©es** : PostgreSQL
- **Framework ML** : Random Forest Regressor
- **API** : FastAPI
- **Authentification** : JWT
- **Conteneurisation** : Docker & Docker Compose
- **Tests** : Pytest

## üöÄ Installation

### Pr√©requis

- Docker & Docker Compose
- Python 3.8+
- 8GB RAM minimum

### Configuration

1. Cloner le d√©p√¥t :
```bash
git clone <repository-url>
cd taxi-eta-prediction
```

2. D√©marrer l'infrastructure :
```bash
docker-compose up -d
```

3. Acc√©der aux services :
- **Interface Airflow** : http://localhost:8080
- **Documentation FastAPI** : http://localhost:8000/docs
- **PostgreSQL** : localhost:5432

4. D√©clencher le DAG Airflow pour d√©marrer le pipeline :
- Ouvrir l'interface Airflow
- Activer et d√©clencher le DAG `taxi_eta_pipeline`

## üìñ Utilisation

### Pipeline Airflow

Le DAG se compose de 5 t√¢ches :

1. **T√©l√©chargement du Dataset** : R√©cup√©ration des donn√©es de trajets de taxi
2. **Stockage Bronze** : Stockage des donn√©es brutes
3. **Traitement Silver** : Nettoyage et enrichissement des donn√©es, stockage dans PostgreSQL
4. **Entra√Ænement du Mod√®le** : Entra√Ænement du mod√®le Random Forest
5. **Logging des Pr√©dictions** : (Bonus) Enregistrement des pr√©dictions pour monitoring

### Effectuer des Pr√©dictions

```python
import requests

# Obtenir un token JWT
response = requests.post("http://localhost:8000/token", 
    data={"username": "user", "password": "password"})
token = response.json()["access_token"]

# Faire une pr√©diction
headers = {"Authorization": f"Bearer {token}"}
trip_data = {
    "trip_distance": 5.2,
    "pickup_hour": 14,
    "day_of_week": 3,
    "passenger_count": 2,
    "PULocationID": 161,
    "DOLocationID": 237
}

response = requests.post("http://localhost:8000/predict", 
    json=trip_data, 
    headers=headers)
print(response.json())
# Sortie : {"estimated_duration": 12.5}
```

## üîå Points de Terminaison API

### Authentification

- `POST /token` - Obtenir un token d'acc√®s JWT

### Pr√©dictions

- `POST /predict` - Obtenir une pr√©diction ETA
  - **Entr√©e** : Caract√©ristiques du trajet (JSON)
  - **Sortie** : `{"estimated_duration": <minutes>}`

### Analytics

#### 1. Dur√©e Moyenne par Heure
- `GET /analytics/avg-duration-by-hour`
- Analyse les heures de pointe en utilisant une CTE SQL
- **R√©ponse** :
```json
[
  {"pickup_hour": 8, "avg_duration": 18.4},
  {"pickup_hour": 9, "avg_duration": 22.1}
]
```

#### 2. Analyse par Type de Paiement
- `GET /analytics/payment-analysis`
- Compare la dur√©e moyenne par type de paiement
- **R√©ponse** :
```json
[
  {
    "payment_type": 1,
    "total_trips": 125430,
    "avg_duration": 21.6
  }
]
```

## üìà Performance du Mod√®le

- **RMSE** : 3.93 minutes
- **Score R¬≤** : 0.803
- **Algorithme** : Random Forest Regressor
- **Gestion des Valeurs Aberrantes** : M√©thode IQR

```

## üß™ Tests

Ex√©cuter les tests unitaires :

```bash
pytest tests/ -v
```

Les tests couvrent :
- Fonctions de pr√©traitement des donn√©es
- Endpoints API
- Authentification
- Pr√©dictions du mod√®le

## üîí S√©curit√©

- Authentification bas√©e sur JWT pour tous les endpoints API
- Gestion s√©curis√©e des credentials via variables d'environnement
- Validation et sanitisation des entr√©es

## üìä Monitoring

Toutes les pr√©dictions sont enregistr√©es dans la table PostgreSQL `eta_predictions` avec :
- Timestamp de la pr√©diction
- Features d'entr√©e
- Dur√©e pr√©dite
- Version du mod√®le

## ü§ù Contribution

1. Forker le d√©p√¥t
2. Cr√©er une branche de fonctionnalit√© (`git checkout -b feature/fonctionnalite-incroyable`)
3. Commiter vos changements (`git commit -m 'Ajout d'une fonctionnalit√© incroyable'`)
4. Pousser vers la branche (`git push origin feature/fonctionnalite-incroyable`)
5. Ouvrir une Pull Request
