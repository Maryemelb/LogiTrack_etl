{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244092dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"first_paquet\").config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\").getOrCreate()\n",
    "df = spark.read.parquet(\"../data/dataset.parquet\")\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f303e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb32d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e64c2e",
   "metadata": {},
   "source": [
    "# Count the number of null values in each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulles\") for c in df.columns]).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bfb9f",
   "metadata": {},
   "source": [
    "# check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af642e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "# pdf['fare_amount'].plot.box()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b378817",
   "metadata": {},
   "source": [
    "# Deal with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e566c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ad69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c766d81",
   "metadata": {},
   "source": [
    "# Create dure_trajet Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af555e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df = df.withColumn(\"dure_trajet\", (sf.unix_timestamp(sf.col(\"tpep_dropoff_datetime\"))- sf.unix_timestamp(sf.col(\"tpep_pickup_datetime\")))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c507aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7195",
   "metadata": {},
   "source": [
    "# Search columns  with the same value duplicates over the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"trip_distance\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc123e2",
   "metadata": {},
   "source": [
    "# Select the Categorial Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98555979",
   "metadata": {},
   "outputs": [],
   "source": [
    "categrial_df= df.select(\"dure_trajet\",\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\",\"payment_type\",\"PULocationID\",\"DOLocationID\")\n",
    "categrial_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39df74",
   "metadata": {},
   "source": [
    "# Analyse the relation between Categorial data and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8199af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categrial_df.plot.countplot(x='VendorID', y='dure_trajet')\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import avg\n",
    "categorical_cols = [\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "   agg = (\n",
    "    categrial_df.groupBy(col).agg(avg(\"dure_trajet\").alias(\"avg_duration\"))\n",
    ")\n",
    "   pdf = agg.toPandas()    \n",
    "   plt.figure(figsize= (25,3))\n",
    "   sns.barplot(data=pdf, x=col, y=\"avg_duration\")\n",
    "   plt.title(f\"Average trip duration by {col}\")\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93309072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "\n",
    "high_categorial_cols= [\"PULocationID\",\"DOLocationID\"]\n",
    "for col in high_categorial_cols:\n",
    "   agg = (\n",
    "    categrial_df.groupBy(col).agg(avg(\"dure_trajet\").alias(\"avg_duration\")).orderBy(desc(\"avg_duration\")).limit(20)\n",
    "    )\n",
    "   pdf = agg.toPandas()    \n",
    "   plt.figure(figsize= (25,3))\n",
    "   sns.barplot(data=pdf, x=col, y=\"avg_duration\")\n",
    "   plt.title(f\"Average trip duration by {col}\")\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b8d42",
   "metadata": {},
   "source": [
    "# Select the Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df= df.select(\"dure_trajet\",\"passenger_count\", \"trip_distance\", \"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"congestion_surcharge\",\"Airport_fee\",\"cbd_congestion_fee\")\n",
    "numerical_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.figure(figsize=(10,7))\n",
    "vector_col= \"assembled_features\"\n",
    "assembler= VectorAssembler(inputCols= numerical_df.columns, outputCol=vector_col, handleInvalid=\"skip\")\n",
    "print(type(assembler))\n",
    "\n",
    "df_vector= assembler.transform(numerical_df).select(vector_col)\n",
    "# df_vector.show(2)\n",
    "corr_matrix= Correlation.corr(df_vector, vector_col)\n",
    "matrix_array = corr_matrix.collect()[0][0].toArray()\n",
    "pd_matrix = pd.DataFrame(matrix_array, columns=numerical_df.columns, index=numerical_df.columns)\n",
    "# print(matrix_array)\n",
    "matrix_df= spark.createDataFrame(pd_matrix, numerical_df.columns)\n",
    "\n",
    "sns.heatmap(pd_matrix, annot=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be7c41",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filter= df.filter((col(\"trip_distance\")> 0) & (col(\"trip_distance\")<200 ) & ( col(\"dure_trajet\") > 0) & (col(\"passenger_count\") > 0))\n",
    "df_filter.show(2)\n",
    "print(df_filter.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77b926",
   "metadata": {},
   "source": [
    "# Look for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a98542",
   "metadata": {},
   "source": [
    "I decided to delete thoese columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8125a",
   "metadata": {},
   "source": [
    "**iqr method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, dayofweek, hour, month, unix_timestamp\n",
    "from pyspark.sql.functions import sin, cos, col, lit\n",
    "import math\n",
    "def data_cleaning(df):\n",
    "      cleaned_df= df.na.drop()\n",
    "      cleaned_df = cleaned_df.withColumn(\"dure_trajet\", (sf.unix_timestamp(sf.col(\"tpep_dropoff_datetime\"))- sf.unix_timestamp(sf.col(\"tpep_pickup_datetime\")))/60)\n",
    "      cleaned_df= cleaned_df.filter((col(\"trip_distance\")> 0) & (col(\"trip_distance\")<200 ) & ( col(\"dure_trajet\") > 0) & (col(\"passenger_count\") > 0))\n",
    "      #drop unicessary columns\n",
    "      cleaned_df =cleaned_df.drop(\"fare_amount\",\"mta_tax\", \"total_amount\",\"cbd_congestion_fee\",\"store_and_fwd_flag\", \"extra\",\"improvement_surcharge\")\n",
    "      \n",
    "      \n",
    "      # cleaned_df = df_filter.withColumn(\"tpep_dropoff_datetime\", (sf.unix_timestamp(sf.col(\"tpep_dropoff_datetime\"))))\n",
    "\n",
    "      \n",
    "      #cleaned_df = cleaned_df.withColumn(\"tpep_pickup_datetime\", (sf.unix_timestamp(sf.col(\"tpep_pickup_datetime\"))))\n",
    "      \n",
    "      #add date columns\n",
    "\n",
    "      cleaned_df= cleaned_df.withColumn(\"dropoff_hour\", hour(\"tpep_dropoff_datetime\"))\\\n",
    "                 .withColumn(\"dropoff_dayofweek\", dayofweek(\"tpep_dropoff_datetime\")) \\\n",
    "                 .withColumn(\"dropoff_month\", month(\"tpep_dropoff_datetime\")) \\\n",
    "                 .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"pickup_dayofweek\", dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"pickup_month\", month(\"tpep_pickup_datetime\"))\n",
    "      cleaned_df=cleaned_df.drop(\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\")\n",
    "      #am not goign to unclude passenger_count because it have limited values in outliers handling\n",
    "      numerical_df_cols= cleaned_df.select(\"dure_trajet\",\"trip_distance\",\"tip_amount\",\"tolls_amount\",\"congestion_surcharge\",\"Airport_fee\")\n",
    "\n",
    "      #handle Outliers\n",
    "      for feature in numerical_df_cols.columns:\n",
    "         quartilles = numerical_df_cols.approxQuantile(feature, [0.25,0.50, 0.75],0) #0 err\n",
    "\n",
    "         iqr = quartilles[2] - quartilles[0]\n",
    "         uper_bound= quartilles[2] + 1.5 * iqr\n",
    "         lower_bound= quartilles[0] - 1.5 * iqr\n",
    "\n",
    "         cleaned_df= cleaned_df.withColumn( \n",
    "              feature,\n",
    "              when(col(feature)>uper_bound , uper_bound) \n",
    "              .when(col(feature) < lower_bound , lower_bound)\n",
    "              .otherwise(col(feature))\n",
    "            \n",
    "         )\n",
    "         \n",
    "         print(feature, quartilles)\n",
    "      return cleaned_df\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = data_cleaning(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b968f",
   "metadata": {},
   "source": [
    "**encode tmestamp features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sin, cos, col, lit\n",
    "import math\n",
    "\n",
    "def add_cyclical_time_features(df):\n",
    "    # Precompute constants\n",
    "    angle_hour = 2 * math.pi / 24\n",
    "    angle_day = 2 * math.pi / 7\n",
    "    angle_month = 2 * math.pi / 12\n",
    "\n",
    "    # Pickup time\n",
    "    df = df.withColumn(\"pickup_hour_sin\", sin(col(\"pickup_hour\") * lit(angle_hour))) \\\n",
    "           .withColumn(\"pickup_hour_cos\", cos(col(\"pickup_hour\") * lit(angle_hour))) \\\n",
    "           .withColumn(\"pickup_dayofweek_sin\", sin((col(\"pickup_dayofweek\") - lit(1)) * lit(angle_day))) \\\n",
    "           .withColumn(\"pickup_dayofweek_cos\", cos((col(\"pickup_dayofweek\") - lit(1)) * lit(angle_day))) \\\n",
    "           .withColumn(\"pickup_month_sin\", sin((col(\"pickup_month\") - lit(1)) * lit(angle_month))) \\\n",
    "           .withColumn(\"pickup_month_cos\", cos((col(\"pickup_month\") - lit(1)) * lit(angle_month)))\n",
    "\n",
    "    # Dropoff time\n",
    "    df = df.withColumn(\"dropoff_hour_sin\", sin(col(\"dropoff_hour\") * lit(angle_hour))) \\\n",
    "           .withColumn(\"dropoff_hour_cos\", cos(col(\"dropoff_hour\") * lit(angle_hour))) \\\n",
    "           .withColumn(\"dropoff_dayofweek_sin\", sin((col(\"dropoff_dayofweek\") - lit(1)) * lit(angle_day))) \\\n",
    "           .withColumn(\"dropoff_dayofweek_cos\", cos((col(\"dropoff_dayofweek\") - lit(1)) * lit(angle_day))) \\\n",
    "           .withColumn(\"dropoff_month_sin\", sin((col(\"dropoff_month\") - lit(1)) * lit(angle_month))) \\\n",
    "           .withColumn(\"dropoff_month_cos\", cos((col(\"dropoff_month\") - lit(1)) * lit(angle_month)))\n",
    "\n",
    "    # Drop original hour/day/month columns\n",
    "    df = df.drop(\"pickup_hour\", \"pickup_dayofweek\", \"pickup_month\",\n",
    "                 \"dropoff_hour\", \"dropoff_dayofweek\", \"dropoff_month\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a126aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = add_cyclical_time_features(df_clean)\n",
    "df_clean.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_clean.columns:\n",
    "    val = df_clean.filter(col(c).isNull()).count()\n",
    "    print(f'{c} : {val} nulls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677f6fc",
   "metadata": {},
   "source": [
    "# download data to postgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext._conf.get(\"spark.jars.packages\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f217ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/taxi_eta\") \\\n",
    "  .option(\"dbtable\", \"taxis\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"password\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95708b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df):\n",
    "    df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/taxi_eta\") \\\n",
    "  .option(\"dbtable\", \"taxis\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"password\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68cdbf2",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "\n",
    "StandardScaler (z-score normalization) is the best choice.\n",
    "\n",
    "Most numeric features (trip_distance, fare_amount, tip_amount, etc.) have different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3769e88",
   "metadata": {},
   "source": [
    "Why scaling IDs actually hurts performance\n",
    "\n",
    "Scaling IDs causes:\n",
    "\n",
    "Artificial ordering\n",
    "\n",
    "Artificial distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyspark Standar scaler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "def normalize(df):\n",
    "    cols_to_scale=[\n",
    "        \"passenger_count\", \"trip_distance\", \"tip_amount\", \"tolls_amount\",\"congestion_surcharge\", \"Airport_fee\"\n",
    "    ]\n",
    "    assembler= VectorAssembler(inputCols= cols_to_scale, outputCol=\"vectors\")\n",
    "    assembled_df =assembler.transform(df)\n",
    "    # assembled_df.show(5)\n",
    "    scaler= StandardScaler(inputCol=\"vectors\", outputCol=\"scaled_features\")\n",
    "    scaler_df= scaler.fit(assembled_df).transform(assembled_df)\n",
    "    cols_to_add_to_features= [\n",
    "          \"DOLocationID\",\"PULocationID\",\n",
    "          \"RatecodeID\", \"VendorID\",\n",
    "          \"payment_type\",\n",
    "          \"scaled_features\",\n",
    "          \"pickup_hour_sin\",\n",
    "         \"pickup_hour_cos\",\n",
    "        \"pickup_dayofweek_sin\",\n",
    "        \"pickup_dayofweek_cos\",\n",
    "        \"pickup_month_sin\",\n",
    "        \"pickup_month_cos\",\n",
    "        \"dropoff_hour_sin\",\n",
    "        \"dropoff_hour_cos\",\n",
    "        \"dropoff_dayofweek_sin\",\n",
    "        \"dropoff_dayofweek_cos\",\n",
    "        \"dropoff_month_sin\",\n",
    "         \"dropoff_month_cos\"\n",
    "         ]\n",
    "    assembler_vector= VectorAssembler(inputCols=cols_to_add_to_features, outputCol=\"features\")\n",
    "    assembled_df_vecotr =assembler_vector.transform(scaler_df)\n",
    "    assembled_df_vecotr =assembled_df_vecotr.select(\"features\", \"dure_trajet\")\n",
    "    return assembled_df_vecotr\n",
    "\n",
    "normalized_df=normalize(df_clean)\n",
    "normalized_df.show(2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5d33c",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7eefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(normalized_df):\n",
    "  train, test= normalized_df.randomSplit([0.7, 0.3])\n",
    "  return train, test\n",
    "train, test = split_data(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e13de3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "def training_rf(train, test):\n",
    "  #set spark logs\n",
    "  # spark.sparkContext.setLogLevel(\"INFO\")\n",
    "  rf= RandomForestRegressor(featuresCol= \"features\", labelCol=\"dure_trajet\", predictionCol=\"prediction_dure\",  numTrees=10,  maxDepth=6)\n",
    "  model=rf.fit(train)\n",
    "  predictions= model.transform(test)\n",
    "  evaluator= RegressionEvaluator(labelCol=\"dure_trajet\", predictionCol=\"prediction_dure\", metricName=\"rmse\")\n",
    "  print(f\"remse:  {evaluator.evaluate(predictions)}\" )\n",
    "\n",
    "  evaluator= RegressionEvaluator(labelCol=\"dure_trajet\", predictionCol=\"prediction_dure\", metricName=\"r2\")\n",
    "  print(f\"re : {evaluator.evaluate(predictions)}\")\n",
    "\n",
    "  return model\n",
    "training_rf(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed0160",
   "metadata": {},
   "source": [
    "## Medallion architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f1e249",
   "metadata": {},
   "source": [
    "## 1. bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f140c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data fct\n",
    "def load_data():\n",
    "    spark = SparkSession.builder.appName(\"first_paquet\").getOrCreate()\n",
    "    df = spark.read.parquet(\"../data/dataset.parquet\")\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4987",
   "metadata": {},
   "source": [
    "## 2.Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40a3d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when \n",
    "\n",
    "def data_cleaning(df):\n",
    "      df_filter= df.filter((col(\"trip_distance\")> 0) & (col(\"trip_distance\")<200 ) & ( col(\"dure_trajet\") > 0) & (col(\"passenger_count\") > 0))\n",
    "      df_filter =df_filter.drop(\"fare_amount\",\"mta_tax\", \"total_amount\",\"cbd_congestion_fee\",\"store_and_fwd_flag\", \"extra\",\"improvement_surcharge\")\n",
    "      #am not goign to unclude passenger_count because it have limited values\n",
    "      numerical_df_cols_no_target= df_filter.select(\"trip_distance\",\"tip_amount\",\"tolls_amount\",\"congestion_surcharge\",\"Airport_fee\")\n",
    "      for feature in numerical_df_cols_no_target.columns:\n",
    "         quartilles = numerical_df_cols_no_target.approxQuantile(feature, [0.25,0.50, 0.75],0) #0 err\n",
    "         iqr = quartilles[2] - quartilles[0]\n",
    "         uper_bound= quartilles[2] + 1.5 * iqr\n",
    "         lower_bound= quartilles[0] - 1.5 * iqr\n",
    "\n",
    "         cleaned_df= df.withColumn( \n",
    "              feature,\n",
    "              when(col(feature)>uper_bound , uper_bound) \n",
    "              .when(col(feature) < lower_bound , lower_bound)\n",
    "              .otherwise(col(feature))\n",
    "         )\n",
    "         print(feature, quartilles)\n",
    "      return cleaned_df\n",
    "      \n",
    "df_clean = data_cleaning(df)\n",
    "#df_clean.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
