{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c9a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244092dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"first_paquet\").config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.3\").getOrCreate()\n",
    "df = spark.read.parquet(\"../data/dataset.parquet\")\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f303e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb32d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e64c2e",
   "metadata": {},
   "source": [
    "# Count the number of null values in each columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c + \"_nulles\") for c in df.columns]).count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bfb9f",
   "metadata": {},
   "source": [
    "# check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af642e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "# pdf['fare_amount'].plot.box()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b378817",
   "metadata": {},
   "source": [
    "# Deal with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e566c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ad69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c766d81",
   "metadata": {},
   "source": [
    "# Create dure_trajet Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af555e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0187d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "df = df.withColumn(\"dure_trajet\", (sf.unix_timestamp(sf.col(\"tpep_dropoff_datetime\"))- sf.unix_timestamp(sf.col(\"tpep_pickup_datetime\")))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c507aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7195",
   "metadata": {},
   "source": [
    "# Search columns  with the same value duplicates over the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4e0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"trip_distance\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc123e2",
   "metadata": {},
   "source": [
    "# Select the Categorial Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98555979",
   "metadata": {},
   "outputs": [],
   "source": [
    "categrial_df= df.select(\"dure_trajet\",\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\",\"payment_type\",\"PULocationID\",\"DOLocationID\")\n",
    "categrial_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39df74",
   "metadata": {},
   "source": [
    "# Analyse the relation between Categorial data and the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8199af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categrial_df.plot.countplot(x='VendorID', y='dure_trajet')\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import avg\n",
    "categorical_cols = [\"VendorID\", \"RatecodeID\", \"store_and_fwd_flag\", \"payment_type\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "   agg = (\n",
    "    categrial_df.groupBy(col).agg(avg(\"dure_trajet\").alias(\"avg_duration\"))\n",
    ")\n",
    "   pdf = agg.toPandas()    \n",
    "   plt.figure(figsize= (25,3))\n",
    "   sns.barplot(data=pdf, x=col, y=\"avg_duration\")\n",
    "   plt.title(f\"Average trip duration by {col}\")\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93309072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, desc\n",
    "\n",
    "high_categorial_cols= [\"PULocationID\",\"DOLocationID\"]\n",
    "for col in high_categorial_cols:\n",
    "   agg = (\n",
    "    categrial_df.groupBy(col).agg(avg(\"dure_trajet\").alias(\"avg_duration\")).orderBy(desc(\"avg_duration\")).limit(20)\n",
    "    )\n",
    "   pdf = agg.toPandas()    \n",
    "   plt.figure(figsize= (25,3))\n",
    "   sns.barplot(data=pdf, x=col, y=\"avg_duration\")\n",
    "   plt.title(f\"Average trip duration by {col}\")\n",
    "   plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b8d42",
   "metadata": {},
   "source": [
    "# Select the Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7944393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_df= df.select(\"dure_trajet\",\"passenger_count\", \"trip_distance\", \"fare_amount\",\"extra\",\"mta_tax\",\"tip_amount\",\"tolls_amount\",\"improvement_surcharge\",\"total_amount\",\"congestion_surcharge\",\"Airport_fee\",\"cbd_congestion_fee\")\n",
    "numerical_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "plt.figure(figsize=(10,7))\n",
    "vector_col= \"assembled_features\"\n",
    "assembler= VectorAssembler(inputCols= numerical_df.columns, outputCol=vector_col, handleInvalid=\"skip\")\n",
    "print(type(assembler))\n",
    "\n",
    "df_vector= assembler.transform(numerical_df).select(vector_col)\n",
    "# df_vector.show(2)\n",
    "corr_matrix= Correlation.corr(df_vector, vector_col)\n",
    "matrix_array = corr_matrix.collect()[0][0].toArray()\n",
    "pd_matrix = pd.DataFrame(matrix_array, columns=numerical_df.columns, index=numerical_df.columns)\n",
    "# print(matrix_array)\n",
    "matrix_df= spark.createDataFrame(pd_matrix, numerical_df.columns)\n",
    "\n",
    "sns.heatmap(pd_matrix, annot=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be7c41",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289c98af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_filter= df.filter((col(\"trip_distance\")> 0) & (col(\"trip_distance\")<200 ) & ( col(\"dure_trajet\") > 0) & (col(\"passenger_count\") > 0))\n",
    "df_filter.show(2)\n",
    "print(df_filter.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77b926",
   "metadata": {},
   "source": [
    "# Look for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a98542",
   "metadata": {},
   "source": [
    "I decided to delete thoese columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d8125a",
   "metadata": {},
   "source": [
    "**iqr method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4c4220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, dayofweek, hour, month, unix_timestamp\n",
    "from pyspark.sql.functions import sin, cos, col, lit\n",
    "import math\n",
    "def data_cleaning(df):\n",
    "      cleaned_df= df.na.drop()\n",
    "      cleaned_df = cleaned_df.withColumn(\"dure_trajet\", (sf.unix_timestamp(sf.col(\"tpep_dropoff_datetime\"))- sf.unix_timestamp(sf.col(\"tpep_pickup_datetime\")))/60)\n",
    "      cleaned_df= cleaned_df.filter((col(\"trip_distance\")> 0) & (col(\"trip_distance\")<200 ) & ( col(\"dure_trajet\") > 0) & (col(\"passenger_count\") > 0))\n",
    "      #drop unicessary columns\n",
    "      cleaned_df =cleaned_df.drop(\"PULocationID\",\"fare_amount\",\"mta_tax\", \"total_amount\",\"cbd_congestion_fee\",\"store_and_fwd_flag\", \"extra\",\"improvement_surcharge\")\n",
    "      \n",
    "      \n",
    "      # cleaned_df = df_filter.withColumn(\"tpep_dropoff_datetime\", (sf.unix_timestamp(sf.col(\"tpep_dropoff_datetime\"))))\n",
    "\n",
    "      \n",
    "      #cleaned_df = cleaned_df.withColumn(\"tpep_pickup_datetime\", (sf.unix_timestamp(sf.col(\"tpep_pickup_datetime\"))))\n",
    "      \n",
    "      #add date columns\n",
    "\n",
    "      cleaned_df= cleaned_df.withColumn(\"dropoff_hour\", hour(\"tpep_dropoff_datetime\"))\\\n",
    "                 .withColumn(\"dropoff_dayofweek\", dayofweek(\"tpep_dropoff_datetime\")) \\\n",
    "                 .withColumn(\"dropoff_month\", month(\"tpep_dropoff_datetime\")) \\\n",
    "                 .withColumn(\"pickup_hour\", hour(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"pickup_dayofweek\", dayofweek(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"pickup_month\", month(\"tpep_pickup_datetime\"))\n",
    "      cleaned_df=cleaned_df.drop(\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\")\n",
    "      #am not goign to unclude passenger_count because it have limited values in outliers handling\n",
    "      numerical_df_cols= cleaned_df.select(\"dure_trajet\",\"trip_distance\",\"tip_amount\",\"tolls_amount\",\"congestion_surcharge\",\"Airport_fee\")\n",
    "\n",
    "      #handle Outliers\n",
    "      for feature in numerical_df_cols.columns:\n",
    "         quartilles = numerical_df_cols.approxQuantile(feature, [0.25,0.50, 0.75],0) #0 err\n",
    "\n",
    "         iqr = quartilles[2] - quartilles[0]\n",
    "         uper_bound= quartilles[2] + 1.5 * iqr\n",
    "         lower_bound= quartilles[0] - 1.5 * iqr\n",
    "\n",
    "         cleaned_df= cleaned_df.withColumn( \n",
    "              feature,\n",
    "              when(col(feature)>uper_bound , uper_bound) \n",
    "              .when(col(feature) < lower_bound , lower_bound)\n",
    "              .otherwise(col(feature))\n",
    "            \n",
    "         )\n",
    "         \n",
    "         print(feature, quartilles)\n",
    "      return cleaned_df\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = data_cleaning(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b968f",
   "metadata": {},
   "source": [
    "**encode tmestamp features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a126aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = add_cyclical_time_features(df_clean)\n",
    "#df_clean.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df_clean.columns:\n",
    "    val = df_clean.filter(col(c).isNull()).count()\n",
    "    print(f'{c} : {val} nulls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677f6fc",
   "metadata": {},
   "source": [
    "# download data to postgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sparkContext._conf.get(\"spark.jars.packages\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f217ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/taxi_eta\") \\\n",
    "  .option(\"dbtable\", \"taxis\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"password\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95708b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df):\n",
    "    df.write \\\n",
    "  .format(\"jdbc\") \\\n",
    "  .option(\"url\", \"jdbc:postgresql://localhost:5432/taxi_eta\") \\\n",
    "  .option(\"dbtable\", \"taxis\") \\\n",
    "  .option(\"user\", \"postgres\") \\\n",
    "  .option(\"password\", \"postgres\") \\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c5d33c",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7eefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(df_clean):\n",
    "  train, test= df_clean.randomSplit([0.7, 0.3])\n",
    "  return train, test\n",
    "train, test = split_data(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e13de3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3c2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f873119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "def training_rf(train, test):\n",
    "  #set spark logs\n",
    "  # spark.sparkContext.setLogLevel(\"INFO\")\n",
    "  cols = [\"VendorID\",\"passenger_count\",\"trip_distance\",\"RatecodeID\",\"DOLocationID\",\"payment_type\",\"tip_amount\",\"tolls_amount\",\"congestion_surcharge\",\"Airport_fee\",\"dure_trajet\",\"dropoff_hour\",\"dropoff_dayofweek\",\"dropoff_month\",\"pickup_hour\",\"pickup_dayofweek\",\"pickup_month\"]\n",
    "  assembler = VectorAssembler(\n",
    "    inputCols=cols,\n",
    "    outputCol=\"features\"\n",
    "  )\n",
    "\n",
    "  train_vec = assembler.transform(train)\n",
    "  test_vec  = assembler.transform(test)\n",
    "  rf= RandomForestRegressor(featuresCol= \"features\", labelCol=\"dure_trajet\", predictionCol=\"prediction_dure\",  numTrees=10,  maxDepth=6)\n",
    "  model=rf.fit(train_vec)\n",
    "  predictions= model.transform(test_vec)\n",
    "  evaluator= RegressionEvaluator(labelCol=\"dure_trajet\", predictionCol=\"prediction_dure\", metricName=\"rmse\")\n",
    "  print(f\"remse:  {evaluator.evaluate(predictions)}\" )\n",
    "\n",
    "  evaluator= RegressionEvaluator(labelCol=\"dure_trajet\", predictionCol=\"prediction_dure\", metricName=\"r2\")\n",
    "  print(f\"re : {evaluator.evaluate(predictions)}\")\n",
    "\n",
    "  return model\n",
    "training_rf(train, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
